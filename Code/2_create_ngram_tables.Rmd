---
title: 'N-Gram Text Prediction Model '
author: "Oli Bailey"
date: "September, Year Of The Virus"
output:
  html_document:
    theme: sandstone
  pdf_document: default
subtitle: 'Step 2. Create Ngram Tables'
---

## NOTES:

This script concerns itself with the creation of trigram, bigram and unigram frequency tables based on our main training corpus generated in the previsou file, "preprocess_corpus.Rmd".

```{r, load_libs, message=FALSE, warning=FALSE}
library(dplyr)      # for wrangling
library(magrittr)   # for piping
library(quanteda)   # for tokenization, ngram creation.
library(tidytext)   # for alternative tokenization, ngram creation.
library(stringr)    # for word to corpus str_c() function
library(qdap)       # for freq_terms() function

# remove any previous variables (clean start)
rm(list = ls())
gc()
```

```{r, utility_functions}
# Get object size in MB
get_MB <- function(obj) {
  format(object.size(obj),units="Mb")
}

# Used for splitting ngrams
get_first_n_words <- function(s,n) {
  str_to_lower(word(s, 1, n))
}

get_last_n_words <- function(s,n) {
  str_to_lower(word(s, -n, -1))
}
```

```{r, load_corpus}
# load "corpus" variable
load("../Data/train_corpus.rdat")
```



Here we'll use the tidytext package to "word" tokenize the corpus, and create ngram tables.

The training set is large, but inevitably, any test set or real input entered by a user will contain words we've never seen before in our model training. We refer to these as out-of-vocabulary ("OOV") words. Our training set will also contain very low frequency ocurrences of obscure words that the user input or a test set is unlikely to ever contain.

We'll handle both these case by replacing unseen (OOV) or low-frequency words in our training set with a token "unkwn", indicating that we are handling an "unknown" word. We need to count the occurences of these to get a feel for the proportion (i.e. probability!) of OOV words compared to in-vocabulary words. 

To this end, we create a unigram table, containing the frequency of all word in the training corpus. We then prune this from about 50,000+ words down to a reasonable 24,000 words by only including words that occur with a frequency of 10 or more in our training corpus. This will also reduce the storage requirement of our ngram tables.

NOTE: These ngram creation chunks run SLOWLY, go and make a nice cup of coffee (or two)...
```{r, create_unigram_vocab}
unigrams <- corpus %>%
  unnest_tokens(text, text, token="ngrams", n=1) %>%
  count(text, sort = TRUE)

# create "in vocab" list
vocab <- unigrams %>% filter(n >= 10)

# how many words in the unigram list are now OOV, i.e. freq n <- 10?
oov_count <- unigrams %>% filter(n < 10) %>% summarise(total = sum(n))

# add "unkwn" to vocab with count to represent all words of low freq as OOV
vocab %<>% add_row(text="unkwn", n=oov_count$total)
vocab %<>% arrange(desc(n))

# modify the corpus with low-frequency OOV words replaced by the "unkwn" token
words <- corpus %>% unnest_tokens(text, text)
words %<>% mutate(text=ifelse(text %in% vocab$text, text, "unkwn"))

# rebuild corpus from words
corpus <- words %>% 
    group_by(line_no) %>% 
    summarize(text = str_c(text, collapse = " ")) %>%
    ungroup()

# vocab replaces unigrams as it now contains an entry for "unkwn"
unigrams <- vocab
head(unigrams,20)   # "unkwn" is 15th most frequenct vocab "item"

# rename "text" column to "ng_next" to match higher order ngrams (below)
names(unigrams)[names(unigrams) == 'text'] <- 'ng_next'
# add "ng_head" column to match higher order ngrams 

unigrams <- cbind(unigrams,ng_head=NA)
# reorder columns to match higher order ng_grams
unigrams %<>% select(n,ng_head,ng_next)

# tidy memory
rm(vocab, words)
gc()

```

Now we have our in-vocabulary list of unigrams, ordered by most frequent first, and our corpus, with low-frequency words considered to be "out-of-vocabulary" and replaced by our "unkwn" token.

We can now create the bigram and trigram tables (again, best to go watch TV or have a coffee, it's slow, at least on my old 3rd gen iCore5 PC...)
```{r, create_higher_order_ngrams}
bigrams <- corpus %>% 
  unnest_tokens(text, text, token="ngrams", n=2) %>%
  count(text, sort = TRUE)

trigrams <- corpus %>% 
  unnest_tokens(text, text, token="ngrams", n=3) %>%
  count(text, sort = TRUE)

quadgrams <- corpus %>% 
  unnest_tokens(text, text, token="ngrams", n=4) %>%
  count(text, sort = TRUE)

# **************** WARNING!!! *************************
# reduce size if you only have 12GB RAM or less like me
# otherwise we run out of memory in later steps... 
#quadgrams  %<>%  filter(n > 1)
#trigrams  %<>%  filter(n > 1)
#bigrams  %<>%  filter(n > 1)

# comment out the above two lines if you have plenty RAM. 
# this "light pruning" causes a small distortion
# of the probabilities...
# *****************************************************


# remove text=NA rows
bigrams %<>% filter(!is.na(text))
trigrams %<>% filter(!is.na(text))
quadgrams %<>% filter(!is.na(text))

get_MB(bigrams); get_MB(trigrams); get_MB(quadgrams) 
```

These bigram and trigrams are large as they contain very low-frequency (i.e. probability) examples that we'll likely never use to predict with, so we will do some pruning to reduce the memory requirement right down to mobile-phone-friendly levels, typically only a few MB.

But first we'll also add a column for the probability of each ngram, in the form of the negative of the natural log, giving us a positive real number we 0 translates to probability of one, and higher numbers represent a logarithmically reducing probability. We'll scale and quantize this to a *positive integer less than about 65,000*  to reduce the ngram object sizes.

```{r, manipulate_and_prune_ngrams}
# split ngrams into "ng_head" and "ng_next" parts
# e.g. a trigrams of words (w1,w2,w3) we split ng_head=(w1,w2), ng_next=(w3)
bigrams %<>% 
  mutate(ng_head=get_first_n_words(text,1),
         ng_next=get_last_n_words(text,1))

trigrams %<>% 
  mutate(ng_head=get_first_n_words(text,2),
         ng_next=get_last_n_words(text,1))

quadgrams %<>% 
  mutate(ng_head=get_first_n_words(text,3),
         ng_next=get_last_n_words(text,1))

# drop full ngram text column to save memory
bigrams %<>% select(-text)
trigrams %<>% select(-text)
quadgrams %<>% select(-text)

# remove rows which have "unkwn" in the predicted word column (ng_next)
# as we never want to offer "unkwn" as a prediction!
bigrams %<>% filter(ng_next != "unkwn")
trigrams %<>% filter(ng_next != "unkwn")
quadgrams %<>% filter(ng_next != "unkwn")
get_MB(bigrams); get_MB(trigrams); get_MB(quadgrams)

# add integer, scaled -log(probabiliity) column to ngrams (more positive -> less likely)
unigrams %<>% mutate(prob = -floor(4000*log(n/sum(n))))
bigrams %<>% group_by(ng_head) %>% mutate(prob = -floor(4000*log(n/sum(n))))
trigrams %<>% group_by(ng_head) %>% mutate(prob = -floor(4000*log(n/sum(n))))
quadgrams %<>% group_by(ng_head) %>% mutate(prob = -floor(4000*log(n/sum(n))))

# arrange tables by increasing prob value (most probable at top of table)
bigrams %<>% arrange(prob)
trigrams %<>% arrange(prob)
quadgrams %<>% arrange(prob)
get_MB(bigrams); get_MB(trigrams); get_MB(quadgrams)

# remove rows after top 6 most frequent ng_next per ng_head ocurrences,
# since we'll never offer these to the user. BIG memory saving...
bigrams %<>% group_by(ng_head) %>% slice(1:6)
trigrams %<>% group_by(ng_head) %>% slice(1:6)
quadgrams %<>% group_by(ng_head) %>% slice(1:6)
get_MB(bigrams); get_MB(trigrams); get_MB(quadgrams)

# remove low frequency ngrams (EVEN BIGGER memory saving!)
# NOTE: tune these carefully to suit memory requirements target, i.e. a few MB each
bigrams   %<>%  filter(n > 2)
trigrams  %<>%  filter(n > 7)
quadgrams  %<>%  filter(n > 4)
get_MB(bigrams); get_MB(trigrams); get_MB(quadgrams)

# reorder unigrams columns to match higher order ngrams
unigrams %<>% select(n, ng_next, prob)

# convert probs to integer explicitly (otherwise it'll be in double float format)
unigrams$prob <- as.integer(unigrams$prob)
bigrams$prob <- as.integer(bigrams$prob)
trigrams$prob <- as.integer(trigrams$prob)
quadgrams$prob <- as.integer(quadgrams$prob)

get_MB(bigrams); get_MB(trigrams); get_MB(quadgrams)

# add empty ng_head column to unigrams tro match higher ngrams
unigrams %<>% mutate(ng_head=NA)
unigrams %<>% select(n,ng_head,ng_next,prob)
```

My model will not use information from any previous sentences to predict the start of the next. Instead we'll just offer the most frequenct sentence starts from our corpora:
```{r, common_line_starts}
(sentence_starts <- str_match(corpus$text, "^[a-z'-]+") %>%
  freq_terms %>% 
  filter(WORD != "na" & WORD != "unkwn") %>%
  head(6))

# number of sentences in train corpus
n <- nrow(corpus)

# add probabilities
sentence_starts %<>% mutate(prob = -floor(4000*log(FREQ/n)))

# format table into same columns as ngram tables (n, ng_next,ng_head,prob)
colnames(sentence_starts) <- c("ng_next", "n","prob")
sentence_starts %<>% mutate(ng_head=NA)
sentence_starts %<>% select(n,ng_head,ng_next,prob)
```

Now save the ngram and sentence start tables for use by the Shiny App.
```{r, save_ngrams_data}
#save ngrams
save(unigrams,bigrams,trigrams,quadgrams, sentence_starts, file = "../Data/ngrams.rdat")
```

Our ngrams up to quadgrams are now saved in an ".rdat" file of only 2.9MB disk space whilst taking about 21MB when in RAM.

The next steps are to prototype a basic model of ngram back off down to the unigram level. We'll not use any smoothing/discounting just yet, just to keep things simple at first, in the next file, "prototype_simple_models.Rmd"
